{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58cc37b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ab9e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SenticNet Dataset\n",
    "def load_senticnet(file_path):\n",
    "    senticnet_df = pd.read_csv(file_path, sep=\",\")\n",
    "    concept_dict = {}\n",
    "\n",
    "    for _, row in senticnet_df.iterrows():\n",
    "        concept = row['CONCEPT']\n",
    "        primary_emotion = row['PRIMARY EMOTION']\n",
    "        polarity_value = row['POLARITY VALUE']\n",
    "        semantics = row['SEMANTICS'].split() if pd.notna(row['SEMANTICS']) else []\n",
    "        concept_dict[concept] = {\n",
    "            'primary_emotion': primary_emotion,\n",
    "            'polarity_value': polarity_value,\n",
    "            'semantics': semantics[:3]  # Top 3 related concepts\n",
    "        }\n",
    "    return concept_dict\n",
    "\n",
    "# Inject Knowledge into Sentence\n",
    "def inject_knowledge(sentence, senticnet):\n",
    "    \"\"\"\n",
    "    Inject knowledge from SenticNet into a sentence and create a structured sentence tree.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    sentence_tree = []\n",
    "\n",
    "    for word in words:\n",
    "        # Fetch knowledge for the word\n",
    "        knowledge = senticnet.get(word, {})\n",
    "        if knowledge:\n",
    "            # Create a node with the wordâ€™s own emotion and polarity\n",
    "            node = {\n",
    "                'word': word,\n",
    "                'primary_emotion': knowledge.get('primary_emotion'),\n",
    "                'polarity_value': knowledge.get('polarity_value'),\n",
    "                'semantics': []\n",
    "            }\n",
    "            # Attach up to three related semantic concepts, each with its own emotion/polarity\n",
    "            for sem_word in knowledge.get('semantics', []):\n",
    "                sem_know = senticnet.get(sem_word, {})\n",
    "                node['semantics'].append({\n",
    "                    'word': sem_word,\n",
    "                    'primary_emotion': sem_know.get('primary_emotion'),\n",
    "                    'polarity_value': sem_know.get('polarity_value'),\n",
    "                })\n",
    "        else:\n",
    "            # If no SenticNet entry exists, fill in with None values\n",
    "            node = {\n",
    "                'word': word,\n",
    "                'primary_emotion': None,\n",
    "                'polarity_value': None,\n",
    "                'semantics': []\n",
    "            }\n",
    "\n",
    "        sentence_tree.append(node)\n",
    "    return sentence_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20434464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for removing noise\n",
    "\n",
    "def assign_soft_positions_and_reorder(sentence_tree):\n",
    "    tokens = []\n",
    "    soft_positions = []\n",
    "    position = 0\n",
    "\n",
    "    # Step 1: Add main words\n",
    "    main_tokens = []\n",
    "    main_positions = []\n",
    "\n",
    "    # Step 2: Collect semantics separately\n",
    "    semantics_tokens = []\n",
    "    semantics_positions = []\n",
    "\n",
    "    for node in sentence_tree:\n",
    "        # Add main word\n",
    "        main_tokens.append(node['word'])\n",
    "        main_positions.append(position)\n",
    "\n",
    "        # Collect semantics (related words)\n",
    "        for semantic in node['semantics']:\n",
    "            semantics_tokens.append(semantic['word'])\n",
    "            semantics_positions.append(position)\n",
    "\n",
    "        position += 1\n",
    "\n",
    "    # Step 3: Reorder by appending semantics after main tokens\n",
    "    tokens = main_tokens + semantics_tokens\n",
    "    soft_positions = main_positions + semantics_positions\n",
    "\n",
    "    return tokens, soft_positions\n",
    "\n",
    "\n",
    "\n",
    "#Visible Matrix\n",
    "def create_visible_matrix_from_tree(sentence_tree):\n",
    "    total_tokens = sum(1 + len(node['semantics']) for node in sentence_tree)\n",
    "    visible_matrix = torch.zeros((total_tokens, total_tokens))\n",
    "\n",
    "    idx = 0\n",
    "    for i, node in enumerate(sentence_tree):\n",
    "        visible_matrix[idx, idx] = 1  # Main word visible to itself\n",
    "        child_start_idx = idx + 1\n",
    "\n",
    "        # Main word visible to its semantics\n",
    "        for j, semantic in enumerate(node['semantics']):\n",
    "            visible_matrix[idx, child_start_idx + j] = 1\n",
    "            visible_matrix[child_start_idx + j, idx] = 1\n",
    "\n",
    "        idx += 1 + len(node['semantics'])\n",
    "\n",
    "    return visible_matrix\n",
    "\n",
    "\n",
    "def pad_visible_matrix(visible_matrix, max_length):\n",
    "    \"\"\"Pad visible matrix to max_length.\"\"\"\n",
    "    padded_matrix = torch.zeros((max_length, max_length))\n",
    "    current_length = visible_matrix.size(0)\n",
    "    padded_matrix[:current_length, :current_length] = visible_matrix\n",
    "    return padded_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bc241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_BERTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, senticnet, max_length=128):\n",
    "        self.data = data  # Accept DataFrame directly\n",
    "        self.tokenizer = tokenizer\n",
    "        self.senticnet = senticnet\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Create a label mapping\n",
    "        unique_labels = sorted(self.data['label'].unique())\n",
    "        self.label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.data['label'] = self.data['label'].map(self.label_mapping)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.iloc[idx]['content']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # Inject SenticNet knowledge\n",
    "        sentence_tree = inject_knowledge(sentence, self.senticnet)\n",
    "        \n",
    "        tokens, soft_positions = assign_soft_positions_and_reorder(sentence_tree)\n",
    "        soft_positions = torch.tensor(soft_positions[:self.max_length], dtype=torch.long)\n",
    "            \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(\n",
    "            tokens,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "\n",
    "        # Create visible matrix\n",
    "        visible_matrix = create_visible_matrix_from_tree(sentence_tree)\n",
    "\n",
    "        # Pad visible matrix to match max_length\n",
    "        padded_visible_matrix = pad_visible_matrix(visible_matrix, self.max_length)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'visible_matrix': padded_visible_matrix,\n",
    "            'soft_positions': soft_positions,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b5fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_visible_matrix(visible_matrix, num_heads):\n",
    "    batch_size, seq_len, seq_len_2 = visible_matrix.size()\n",
    "    expanded_matrix = visible_matrix.unsqueeze(1).repeat(1, num_heads, 1, 1)\n",
    "    return expanded_matrix.view(batch_size * num_heads, seq_len, seq_len)\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_attention_heads)\n",
    "    \n",
    "    def forward(self, inputs, attention_mask, visible_matrix):\n",
    "        batch_size, seq_len, hidden_size = inputs.size()\n",
    "\n",
    "        # Incorporate attention_mask into visible_matrix:\n",
    "        # attention_mask: [batch_size, seq_len], 1 for real tokens, 0 for padded\n",
    "        # Make broadcastable\n",
    "        valid_tokens = attention_mask.unsqueeze(-1) * attention_mask.unsqueeze(1)\n",
    "        combined_matrix = visible_matrix * valid_tokens\n",
    "\n",
    "        # Expand for multi-head\n",
    "        combined_matrix = expand_visible_matrix(combined_matrix, self.num_attention_heads)\n",
    "\n",
    "        # Create attn_mask with a large negative number instead of -inf\n",
    "        attn_mask = torch.zeros_like(combined_matrix, dtype=torch.float)\n",
    "        attn_mask[combined_matrix == 0] = -1e9  # large negative number\n",
    "\n",
    "        # inputs: [batch_size, seq_len, hidden_size] -> [seq_len, batch_size, hidden_size]\n",
    "        inputs = inputs.permute(1, 0, 2)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        # IMPORTANT: multihead attention expects attn_mask to be [L, S] or [N*num_heads, L, S].\n",
    "        # We have [N*num_heads, seq_len, seq_len], which should be correct since we repeated for heads.\n",
    "        outputs, _ = self.attention(inputs, inputs, inputs, attn_mask=attn_mask)\n",
    "        \n",
    "        # [seq_len, batch_size, hidden_size] -> [batch_size, seq_len, hidden_size]\n",
    "        outputs = outputs.permute(1, 0, 2)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabf60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_RoBERTa(nn.Module):\n",
    "    def __init__(self, transformer, hidden_size, num_attention_heads, num_labels, max_length=128):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer  # Use RobertaForSequenceClassification\n",
    "        self.masked_attention = MaskedSelfAttention(hidden_size, num_attention_heads)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.soft_position_embeddings = nn.Embedding(max_length, hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, visible_matrix, soft_positions):\n",
    "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        # print(\"After transformer:\", outputs.logits if hasattr(outputs, 'logits') else outputs.hidden_states[-1])\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        # print(\"hidden_states check NaN:\", torch.isnan(hidden_states).any())\n",
    "        soft_position_embeds = self.soft_position_embeddings(soft_positions)\n",
    "\n",
    "        normalized_hidden_states = self.layer_norm(hidden_states)\n",
    "        # print(\"normalized hidden_states check NaN:\", torch.isnan(normalized_hidden_states).any())\n",
    "\n",
    "        masked_outputs = self.masked_attention(normalized_hidden_states, attention_mask, visible_matrix)\n",
    "        # print(\"masked_outputs check NaN:\", torch.isnan(masked_outputs).any())\n",
    "\n",
    "        pooled_output = masked_outputs.mean(dim=1)\n",
    "        # print(\"pooled_output check NaN:\", torch.isnan(pooled_output).any())\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "        # print(\"classifier logits check NaN:\", torch.isnan(logits).any())\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ced7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    visible_matrices = [item['visible_matrix'] for item in batch]\n",
    "    soft_positions = [item['soft_positions'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    visible_matrices = pad_sequence(visible_matrices, batch_first=True, padding_value=0)\n",
    "    soft_positions = pad_sequence(soft_positions, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)  # stack them into a single tensor\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'visible_matrix': visible_matrices,\n",
    "        'soft_positions': soft_positions,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c245c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "root_path = \"cleaned_full_isear_data_with_sentiment.csv\"\n",
    "df = pd.read_csv(os.path.join(root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a9403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {2: 0, 11: 1, 14: 2, 17: 3, 25: 4, 28: 5, 29: 6}\n",
      "Mapped labels: [3 2 0 4 1 5 6]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "unique_labels = sorted(df['label'].unique())\n",
    "label_mapping = {original_label: idx for idx, original_label in enumerate(unique_labels)}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "# Print to verify the mapping\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "print(\"Mapped labels:\", df['label'].unique())\n",
    "\n",
    "num_labels = len(unique_labels)\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0172f334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5334, Validation size: 593, Test size: 1482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/syoon333/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "K_RoBERTa(\n",
       "  (transformer): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (masked_attention): MaskedSelfAttention(\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (soft_position_embeddings): Embedding(128, 768)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Load the SenticNet Data\n",
    "senticnet_path = \"senticnet/senticnet.csv\"\n",
    "senticnet_data = load_senticnet(senticnet_path)\n",
    "\n",
    "# 2. Load and Split the Dataset\n",
    "data_path = \"cleaned_full_isear_data_with_sentiment.csv\"\n",
    "full_data = pd.read_csv(data_path)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "train_data, dev_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(dev_data)}, Test size: {len(test_data)}\")\n",
    "\n",
    "#Tokenizer\n",
    "pretrained_model_path = \"results/checkpoint-3250\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_path)\n",
    "transformer = RobertaForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_path, num_labels=num_labels, output_hidden_states=True\n",
    ")\n",
    "\n",
    "#Create K_BERTDataset Instances\n",
    "train_dataset = K_BERTDataset(train_data, tokenizer, senticnet_data)\n",
    "dev_dataset = K_BERTDataset(dev_data, tokenizer, senticnet_data)\n",
    "test_dataset = K_BERTDataset(test_data, tokenizer, senticnet_data)\n",
    "\n",
    "#Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "num_attention_heads = 12  # Default number of attention heads for Roberta\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = K_RoBERTa(\n",
    "    transformer = RobertaForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_path, num_labels=num_labels, output_hidden_states=True\n",
    "),\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    num_labels=num_labels,\n",
    "    hidden_size = transformer.config.hidden_size\n",
    ")\n",
    "\n",
    "#Optimizer and Loss Function\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-7,  # Reduced learning rate for stability\n",
    "    weight_decay=1e-4,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Training Mode\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b348c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c398f2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 26.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss: 1.742286432645992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Average Loss: 1.3846035565801724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Average Loss: 1.1133766085087895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Average Loss: 0.9318083389076645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Average Loss: 0.816101782603892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Average Loss: 0.741339543800868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Average Loss: 0.6913469489106161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Average Loss: 0.6466747961358396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Average Loss: 0.6112514674485087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Average Loss: 0.5946847325313591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Average Loss: 0.5662804308854891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Average Loss: 0.5387972978551587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Average Loss: 0.5312360629305511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Average Loss: 0.5186238767590351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 26.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Average Loss: 0.5037548653558342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Average Loss: 0.49079915609009966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Average Loss: 0.48228908624656186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Average Loss: 0.4600169605012247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Average Loss: 0.46345898573716243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Average Loss: 0.44164441151176387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Average Loss: 0.4351989479211276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Average Loss: 0.42569702150980515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Average Loss: 0.4174324171673395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Average Loss: 0.41034732544850444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Average Loss: 0.4030585873002064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Average Loss: 0.3900498673877495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Average Loss: 0.380885520816117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Average Loss: 0.3775050260215819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Average Loss: 0.3646011140509815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Average Loss: 0.36227137453764857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Average Loss: 0.3662828447486826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Average Loss: 0.34460380800559137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Average Loss: 0.3422045748859287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Average Loss: 0.34626887467673084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Average Loss: 0.3296638093554153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Average Loss: 0.31420751386126894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Average Loss: 0.31880813682016856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Average Loss: 0.31266475636735114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Average Loss: 0.30393786936953754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Average Loss: 0.3062571483123267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Average Loss: 0.29052861909309546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Average Loss: 0.2945202805525707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Average Loss: 0.291099814675793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Average Loss: 0.28643781445853544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Average Loss: 0.27569911700344374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Average Loss: 0.2709143208674417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Average Loss: 0.2608676276666051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 26.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Average Loss: 0.25702353410177425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Average Loss: 0.25774772021874875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 334/334 [00:12<00:00, 27.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Average Loss: 0.2617207909571732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        visible_matrix = batch['visible_matrix'].to(device)\n",
    "        soft_positions = batch['soft_positions'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "#         if torch.isnan(input_ids).any() or torch.isnan(visible_matrix).any():\n",
    "#             print(\"NaN in inputs before forward pass\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, visible_matrix, soft_positions)\n",
    "        \n",
    "#         if torch.isnan(logits).any():\n",
    "#             print(\"NaN detected in logits before loss computation\")\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Gradient clipping to avoid parameter explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6530cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            visible_matrix = batch['visible_matrix'].to(device)\n",
    "            soft_positions = batch['soft_positions'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            \n",
    "            logits = model(input_ids, attention_mask, visible_matrix, soft_positions)\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.tolist())\n",
    "            all_preds.extend(preds.tolist())\n",
    "      \n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted') \n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')        \n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f684c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:01<00:00, 59.21batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-BERT with SenticNet Accuracy: 88.06%\n",
      "K-BERT Mean Precision: 88.09%\n",
      "K-BERT Mean Recall: 88.06%\n",
      "K-BERT Mean F1 Score: 88.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kbert_accuracy, kbert_precision, kbert_recall, kbert_f1 = evaluate_model(model, test_loader)\n",
    "print(f\"K-BERT with SenticNet Accuracy: {kbert_accuracy * 100:.2f}%\")\n",
    "print(f\"K-BERT Mean Precision: {kbert_precision * 100:.2f}%\")\n",
    "print(f\"K-BERT Mean Recall: {kbert_recall * 100:.2f}%\")\n",
    "print(f\"K-BERT Mean F1 Score: {kbert_f1 * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
